{"paragraphs":[{"text":"case class policies(\nagricultureStructureIndicator: String,\nbaseFloodElevation: Integer,\nbasementEnclosureCrawlspace: Integer,\ncensusTract: String,\ncancellationDateOfFloodPolicy: String,\ncondominiumIndicator: String,\nconstruction: String,\ncountyCode: String,\ncrsClassCode: String,\ndeductibleAmountInBuildingCoverage: String,\ndeductibleAmountInContentsCoverage: String,\nelevationBuildingIndicator: String,\nelevationCertificateIndicator: String,\nelevationDifference: String,\nfederalPolicyFee: String,\nfloodZone: String,\nhfiaaSurcharge: Integer,\nhouseOfWorshipIndicator: String,\nlatitude: String,\nlongitude: String,\nlocationOfContents: String,\nlowestAdjacentGrade: Double,\nlowestFloorElevation: Double,\nnonProfitIndicator: String,\nnumberOfFloorsInTheInsuredBuilding: Integer,\nobstructionType: String,\noccupancyType: String,\noriginalConstructionDate: String,\noriginalNBDate: String,\npolicyCost: Integer,\npolicyCount: Integer,\npolicyEffectiveDate: String,\npolicyTerminationDate: String,\npolicyTermIndicator: String,\npostFIRMConstructionIndicator: String,\nprimaryResidenceIndicator: String,\npropertyState: String,\nreportedZipCode: String,\nrateMethod: String,\nregularEmergencyProgramIndicator: String,\nreportedCity: String,\nsmallBusinessIndicatorBuilding: String,\ntotalBuildingInsuranceCoverage: Double,\ntotalContentsInsuranceCoverage: Double,\ntotalInsurancePremiumOfThePolicy: Double,\nid: String)\n\ncase class claims(\nagricultureStructureIndicator: String,\nasOfDate: String,\nbaseFloodElevation: Integer,\nbasementEnclosureCrawlspace: String,\nreportedCity: String,\ncondominiumIndicator: String,\npolicyCount: String,\ncountyCode: String,\ncommunityRatingSystemDiscount: String,\ndateOfLoss: String,\nelevatedBuildingIndicator: String,\nelevationCertificateIndicator: String,\nelevationDifference: String,\ncensusTract: String,\nfloodZone: String,\nhouseWorship: String,\nlatitude: String,\nlongitude: String,\nlocationOfContents: String,\nlowestAdjacentGrade: Double,\nlowestFloorElevation: Double,\nnumberOfFloorsInTheInsuredBuilding: Integer,\nnonProfitIndicator: String,\nobstructionType: String,\noccupancyType: String,\noriginalConstructionDate: String,\noriginalNBDate: String,\namountPaidOnBuildingClaim: String,\namountPaidOnContentsClaim: String,\namountPaidOnIncreasedCostOfComplianceClaim: String,\npostFIRMConstructionIndicator: String,\nrateMethod: String,\nsmallBusinessIndicatorBuilding: String,\nstate: String,\ntotalBuildingInsuranceCoverage: String,\ntotalContentsInsuranceCoverage: String,\nyearOfLoss: String,\nreportedZipcode: String,\nprimaryResidence: String,\nid:String)\n\nimport org.apache.spark.sql._\n\n//load datasets\n\nval policy_ds_orig: Dataset[policies] = spark.sqlContext.read.option(\"header\", \"true\").option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").csv(\"/user/maria_dev/FimaNfipPolicies.csv\").as[policies]\nval claims_ds_orig: Dataset[claims] = spark.sqlContext.read.option(\"header\", \"true\").option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").csv(\"/user/maria_dev/FimaNfipClaims.csv\").as[claims]\n\n//val policy_ds_orig: Dataset[policies] = spark.sqlContext.read.option(\"header\", \"true\").option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").csv(\"/user/maria_dev/final/policies_small.csv\").as[policies]\n//val claims_ds_orig: Dataset[claims] = spark.sqlContext.read.option(\"header\", \"true\").option(\"delimiter\", \",\").option(\"inferSchema\", \"true\").csv(\"/user/maria_dev/final/claims_small.csv\").as[claims]\n\n//replace blank policy counts for claims data with value of 1 (must have at least 1 policy to have a claim)\n//replace blank claim value fields with 0's\nval claims_ds_clean = claims_ds_orig.withColumn(\"policyCount\", when($\"policyCount\".isNull, lit(1)).otherwise($\"policyCount\")).withColumn(\"amountPaidOnBuildingClaim\", when($\"amountPaidOnBuildingClaim\".isNull, 0).otherwise($\"amountPaidOnBuildingClaim\")).withColumn(\"amountPaidOnContentsClaim\", when($\"amountPaidOnContentsClaim\".isNull, 0).otherwise($\"amountPaidOnContentsClaim\")).withColumn(\"amountPaidOnIncreasedCostOfComplianceClaim\", when($\"amountPaidOnIncreasedCostOfComplianceClaim\".isNull, 0).otherwise($\"amountPaidOnIncreasedCostOfComplianceClaim\"))\n\n//replace blank policy value columns with zero\nval policy_ds_clean = policy_ds_orig.withColumn(\"totalBuildingInsuranceCoverage\", when($\"totalBuildingInsuranceCoverage\".isNull, 0).otherwise($\"totalBuildingInsuranceCoverage\")).withColumn(\"totalContentsInsuranceCoverage\", when($\"totalContentsInsuranceCoverage\".isNull, 0).otherwise($\"totalContentsInsuranceCoverage\")).withColumn(\"policyCost\", when($\"policyCost\".isNull, 0).otherwise($\"policyCost\"))\n\n//Entries with fully blank amounts will be filtered out later. Retain rows for count related questions.","user":"anonymous","dateUpdated":"2021-04-11T19:52:51+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class policies\ndefined class claims\nimport org.apache.spark.sql._\npolicy_ds_orig: org.apache.spark.sql.Dataset[policies] = [agricultureStructureIndicator: int, baseFloodElevation: int ... 44 more fields]\nclaims_ds_orig: org.apache.spark.sql.Dataset[claims] = [agricultureStructureIndicator: int, asOfDate: timestamp ... 38 more fields]\nclaims_ds_clean: org.apache.spark.sql.DataFrame = [agricultureStructureIndicator: int, asOfDate: timestamp ... 38 more fields]\npolicy_ds_clean: org.apache.spark.sql.DataFrame = [agricultureStructureIndicator: int, baseFloodElevation: int ... 44 more fields]\n"}]},"apps":[],"jobName":"paragraph_1618161235266_280999320","id":"20210411-171355_1374772323","dateCreated":"2021-04-11T17:13:55+0000","dateStarted":"2021-04-11T19:52:51+0000","dateFinished":"2021-04-11T19:55:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6122"},{"text":"//1. Which states have the highest and lowest claim (# of claims) to policy (# of policies) ratios? Consider policyCount field.\n\nval grouped_policy_ds = policy_ds_clean.groupBy(\"propertyState\").agg(sum(\"policycount\") as \"policy_counts\")\nval grouped_claims_ds = claims_ds_clean.groupBy(\"state\").agg(sum(\"policyCount\") as \"claim_counts\")\nval joined_grouped_ds = grouped_policy_ds.join(grouped_claims_ds, grouped_policy_ds(\"propertyState\") === grouped_claims_ds(\"state\"))\nval ratio_ds = joined_grouped_ds.withColumn(\"claims_to_policies\", $\"claim_counts\"/$\"policy_counts\").drop(\"state\").orderBy(\"claims_to_policies\").show()\nval ratio_ds_desc = joined_grouped_ds.withColumn(\"claims_to_policies\", $\"claim_counts\"/$\"policy_counts\").drop(\"state\").orderBy(desc(\"claims_to_policies\")).show()","user":"anonymous","dateUpdated":"2021-04-11T19:55:15+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"grouped_policy_ds: org.apache.spark.sql.DataFrame = [propertyState: string, policy_counts: bigint]\ngrouped_claims_ds: org.apache.spark.sql.DataFrame = [state: string, claim_counts: bigint]\njoined_grouped_ds: org.apache.spark.sql.DataFrame = [propertyState: string, policy_counts: bigint ... 2 more fields]\n+-------------+-------------+------------+--------------------+\n|propertyState|policy_counts|claim_counts|  claims_to_policies|\n+-------------+-------------+------------+--------------------+\n|           NM|        28543|        1425|0.049924675051676415|\n|           ID|        12116|        1126| 0.09293496203367448|\n|           NV|        23430|        2273| 0.09701237729406743|\n|           OR|        66438|        6687| 0.10065023028989434|\n|           HI|       181549|       19168|  0.1055803116513999|\n|           CA|       487248|       53203| 0.10919080221981414|\n|           AZ|        36229|        5055| 0.13952910651687875|\n|           CO|        45894|        6627| 0.14439796051771472|\n|           WY|         3747|         545|  0.1454496930878036|\n|           MT|        12607|        2114|  0.1676846196557468|\n|           UT|         6520|        1116| 0.17116564417177915|\n|           GA|       133258|       24796| 0.18607513244983415|\n|           FL|      3572104|      696527|  0.1949906833619626|\n|           WA|        71457|       15043| 0.21051821375092714|\n|           AK|         3252|         694|  0.2134071340713407|\n|           DC|         4173|         959|   0.229810687754613|\n|           WI|        38224|        9024| 0.23608204269568858|\n|           DE|        37394|        9044| 0.24185698240359416|\n|           NH|        16954|        4667| 0.27527427155833434|\n|           NE|        21519|        6052| 0.28123983456480317|\n+-------------+-------------+------------+--------------------+\nonly showing top 20 rows\n\nratio_ds: Unit = ()\n+-------------+-------------+------------+-------------------+\n|propertyState|policy_counts|claim_counts| claims_to_policies|\n+-------------+-------------+------------+-------------------+\n|           PR|        17241|       28115| 1.6307058755292616|\n|           VI|         2919|        4636| 1.5882151421719768|\n|           AL|        55945|       86476|  1.545732415765484|\n|           MO|        44982|       50797| 1.1292739317949403|\n|           WV|        24298|       26990| 1.1107910116058934|\n|           KY|        26703|       24728| 0.9260382728532375|\n|           AS|           37|          30| 0.8108108108108109|\n|           NC|       170910|      126694| 0.7412907378152244|\n|           MN|        18895|       12518|  0.662503307753374|\n|           SC|       111177|       71886| 0.6465905717909279|\n|           IA|        24541|       14765| 0.6016462246852207|\n|           LA|       801107|      474692| 0.5925450657652473|\n|           IL|        96052|       56551| 0.5887540082455337|\n|           NJ|       417143|      241585| 0.5791419249513956|\n|           NY|       323966|      187059| 0.5774031842847707|\n|           ND|        24483|       13580| 0.5546705877547686|\n|           PA|       143193|       77669| 0.5424077992639305|\n|           OK|        25658|       12858|  0.501130251773326|\n|           MS|       140461|       66962|0.47673019557030066|\n|           SD|         8718|        4051|0.46467079605414086|\n+-------------+-------------+------------+-------------------+\nonly showing top 20 rows\n\nratio_ds_desc: Unit = ()\n"}]},"apps":[],"jobName":"paragraph_1618161257423_2053784334","id":"20210411-171417_1990638566","dateCreated":"2021-04-11T17:14:17+0000","dateStarted":"2021-04-11T19:55:15+0000","dateFinished":"2021-04-11T19:56:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6123"},{"text":"//2. What is the average total (building and contents) policy, average premium (total policy cost), and average claim amount for each occupancy type and the ratio of claim payout to policy amount and the ratio of claim payout to premium amount\n//One-digit code: 1 = single family residence; 2 = 2 to 4 unit residential building; 3 = residential building with more than 4 units; 4 = Non-residential building; 6 = Non-Residential Business\n\n//only include claims and policies that are non-zero\nval policy_ds_filtered = policy_ds_clean.filter($\"policyCost\".cast(\"Integer\") > lit(0))\nval claims_ds_filtered = claims_ds_clean.filter($\"amountPaidOnBuildingClaim\".cast(\"Integer\") +$\"amountPaidOnContentsClaim\".cast(\"Integer\") + $\"amountPaidOnIncreasedCostOfComplianceClaim\".cast(\"Integer\") > lit(0))\n\nval grouped_policy_ds2 = policy_ds_filtered.groupBy(\"occupancyType\").agg(avg($\"totalBuildingInsuranceCoverage\".cast(\"Integer\") + $\"totalContentsInsuranceCoverage\".cast(\"Integer\")) as \"average_policy_amount\", avg($\"policyCost\") as \"average_premium\")\nval grouped_claims_ds2 = claims_ds_filtered.groupBy(\"occupancyType\").agg(avg($\"amountPaidOnBuildingClaim\".cast(\"Integer\") +$\"amountPaidOnContentsClaim\".cast(\"Integer\") + $\"amountPaidOnIncreasedCostOfComplianceClaim\".cast(\"Integer\")) as \"average_claim_payout\")\nval joined_grouped_ds2 = grouped_policy_ds2.join(grouped_claims_ds2, grouped_policy_ds2(\"occupancyType\") === grouped_claims_ds2(\"occupancyType\"))\nval ratio_ds2 = joined_grouped_ds2.withColumn(\"claims_to_total_policies\", $\"average_claim_payout\"/$\"average_policy_amount\").withColumn(\"claims_to_premiums\", $\"average_claim_payout\"/$\"average_premium\").orderBy($\"claims_to_premiums\").show()","user":"anonymous","dateUpdated":"2021-04-11T19:56:35+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"policy_ds_filtered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [agricultureStructureIndicator: int, baseFloodElevation: int ... 44 more fields]\nclaims_ds_filtered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [agricultureStructureIndicator: int, asOfDate: timestamp ... 38 more fields]\ngrouped_policy_ds2: org.apache.spark.sql.DataFrame = [occupancyType: int, average_policy_amount: double ... 1 more field]\ngrouped_claims_ds2: org.apache.spark.sql.DataFrame = [occupancyType: int, average_claim_payout: double]\njoined_grouped_ds2: org.apache.spark.sql.DataFrame = [occupancyType: int, average_policy_amount: double ... 3 more fields]\n+-------------+---------------------+------------------+-------------+--------------------+------------------------+------------------+\n|occupancyType|average_policy_amount|   average_premium|occupancyType|average_claim_payout|claims_to_total_policies|claims_to_premiums|\n+-------------+---------------------+------------------+-------------+--------------------+------------------------+------------------+\n|            4|   393511.03565984865|2330.9442786342106|            4|  47307.413376759854|     0.12021877174914208| 20.29538578437365|\n|            3|   1221096.7684416568|2567.0117729422354|            3|   65743.36466845161|    0.053839602534000795|25.610854364371868|\n|            2|    286211.5699392988| 1170.916065878838|            2|   30650.53544509279|     0.10709048362927227|26.176543595452205|\n|            6|    423769.7340824994| 2692.806217433468|            6|   95443.64460706331|      0.2252252507218516| 35.44393354009383|\n|            1|    262711.8620184846| 761.1956476680173|            1|   35089.66069426515|     0.13356709676016157|46.098083721005345|\n+-------------+---------------------+------------------+-------------+--------------------+------------------------+------------------+\n\nratio_ds2: Unit = ()\n"}]},"apps":[],"jobName":"paragraph_1618161269833_133887322","id":"20210411-171429_1810159173","dateCreated":"2021-04-11T17:14:29+0000","dateStarted":"2021-04-11T19:56:35+0000","dateFinished":"2021-04-11T19:57:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6124"},{"text":"//3. Which zipcodes had more than 5000 claims in any given year (ignoring policy count)\nval filtered_claims_ds3 = claims_ds_clean.filter($\"reportedZipcode\".isNotNull)\nval grouped_claims_ds3 = filtered_claims_ds3.groupBy(\"reportedZipcode\",\"yearOfLoss\").agg(count($\"policyCount\") as \"claim_count\")\nval filtered_claims_ds3 = grouped_claims_ds3.filter($\"claim_count\".cast(\"Integer\") > 5000).orderBy(desc(\"claim_count\")).show(50,false)","user":"anonymous","dateUpdated":"2021-04-11T19:58:09+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filtered_claims_ds3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [agricultureStructureIndicator: int, asOfDate: timestamp ... 38 more fields]\ngrouped_claims_ds3: org.apache.spark.sql.DataFrame = [reportedZipcode: int, yearOfLoss: int ... 1 more field]\n+---------------+----------+-----------+\n|reportedZipcode|yearOfLoss|claim_count|\n+---------------+----------+-----------+\n|70065          |2005      |10783      |\n|8008           |2012      |9894       |\n|70122          |2005      |9638       |\n|70043          |2005      |8546       |\n|70003          |2005      |8443       |\n|70126          |2005      |8092       |\n|70124          |2005      |7551       |\n|70458          |2005      |7375       |\n|70119          |2005      |6936       |\n|11561          |2012      |6551       |\n|70072          |2005      |6218       |\n|70117          |2005      |5886       |\n|70127          |2005      |5540       |\n|70058          |2005      |5527       |\n|70001          |2005      |5514       |\n|70128          |2005      |5482       |\n|77550          |2008      |5410       |\n|8226           |2012      |5365       |\n+---------------+----------+-----------+\n\nfiltered_claims_ds3: Unit = ()\n"}]},"apps":[],"jobName":"paragraph_1618161283175_2016109590","id":"20210411-171443_1232081736","dateCreated":"2021-04-11T17:14:43+0000","dateStarted":"2021-04-11T19:58:09+0000","dateFinished":"2021-04-11T19:58:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6125"},{"text":"//4. Which city had the most loss for each state for each decade?\n\n//val grouped_claims_ds4 = claims_ds_clean.groupBy(\"reportedCity\",\"state\").agg(count($\"policyCount\") as \"claim_count\").orderBy(\"claim_count\",\"state\").show(50,false)\n\n//only include claims that are non-zero\nval claims_ds_filtered = claims_ds_clean.filter($\"amountPaidOnBuildingClaim\".cast(\"Integer\") +$\"amountPaidOnContentsClaim\".cast(\"Integer\") + $\"amountPaidOnIncreasedCostOfComplianceClaim\".cast(\"Integer\") > lit(0))\n\n//Add a decade column. Start in 1980 as this is the first full decade of data and end in 2019 for the last full decade of data.\nval claims_ds4_decades = claims_ds_filtered.withColumn(\"Decade\", when($\"yearOfLoss\" >= 1980 && $\"yearOfLoss\" <1990, \"1980s\").when($\"yearOfLoss\" >= 1990 && $\"yearOfLoss\" <2000, \"1990s\").when($\"yearOfLoss\" >= 2000 && $\"yearOfLoss\" <2010, \"2000s\").when($\"yearOfLoss\" >= 2010 && $\"yearOfLoss\" <2020, \"2010s\").otherwise(lit(\"outOfRange\"))).filter($\"Decade\" =!= \"outOfRange\")\n\nval grouped_claims_ds4b = claims_ds4_decades.groupBy(\"reportedCity\",\"state\",\"decade\").agg(sum($\"amountPaidOnBuildingClaim\".cast(\"Integer\") +$\"amountPaidOnContentsClaim\".cast(\"Integer\") + $\"amountPaidOnIncreasedCostOfComplianceClaim\".cast(\"Integer\")) as \"total_loss\").orderBy(desc(\"total_loss\"))\nval max_claims = grouped_claims_ds4b.groupBy(\"decade\").agg(max($\"total_loss\") as \"max_total_loss\")\n\nval joined = max_claims.join(grouped_claims_ds4b , max_claims(\"max_total_loss\") === grouped_claims_ds4b(\"total_loss\")).drop(\"max_total_loss\").orderBy(\"total_loss\").show(10,false)\n","user":"anonymous","dateUpdated":"2021-04-11T19:59:32+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"claims_ds_filtered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [agricultureStructureIndicator: int, asOfDate: timestamp ... 38 more fields]\nclaims_ds4_decades: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [agricultureStructureIndicator: int, asOfDate: timestamp ... 39 more fields]\ngrouped_claims_ds4b: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [reportedCity: string, state: string ... 2 more fields]\nmax_claims: org.apache.spark.sql.DataFrame = [decade: string, max_total_loss: bigint]\n+------+------------+-----+------+----------+\n|decade|reportedCity|state|Decade|total_loss|\n+------+------------+-----+------+----------+\n|1980s |HOUSTON     |TX   |1980s |136540492 |\n|1990s |NEW ORLEANS |LA   |1990s |227341928 |\n|2010s |HOUSTON     |TX   |2010s |4498711000|\n|2000s |NEW ORLEANS |LA   |2000s |6877078312|\n+------+------------+-----+------+----------+\n\njoined: Unit = ()\n"}]},"apps":[],"jobName":"paragraph_1618161291332_-1123826184","id":"20210411-171451_281833229","dateCreated":"2021-04-11T17:14:51+0000","dateStarted":"2021-04-11T19:59:32+0000","dateFinished":"2021-04-11T20:00:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6126"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1618170159052_-829930943","id":"20210411-194239_2115718444","dateCreated":"2021-04-11T19:42:39+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6959"}],"name":"ProjectSixPart3","id":"2G26H91C1","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}